---
layout: posts
mathjax: true
title: Laplacian Pyramid GAN
categories:
 - 논문 구현
tags:
 - GAN
 - DeepLearning
 - Generative model
toc: true
toc_sticky: true
author_profile: true
sidebar:
  nav: "categories"
---


참고 논문: [Laplacian Pyramid GAN.pdf](https://github.com/KimSungHeon/KimSungHeon.github.io/files/11131347/Laplacian.Pyramid.GAN.pdf)


Laplacian Pyramid GAN (a.k.a LAPGAN)은 GAN모델을 기반으로 Pyramid처럼 이미지의 크기를 늘려나가는데, 다른 크기의 이미지에서 서로 다른 정보를 모아 더 나은 이미지를 생성하는 모델이다. 

학습 과정에서, 각 피라미드의 Level(피라미드의 층)마다 GAN모듈이 하나씩 들어가있으며, level이 진행됨에 따라 이미지의 크기가
upsampling 되며, 최종적으로는 원본 이미지와 크기가 동일해진다.


# Laplacian Pyramid

Laplacian pyramid는 이미지를 선형적으로 표현하는데 사용되는 방법이다. Gaussian pyramid 에서는 이미지를 downsampling하여 크기를 줄이고 blurring하여 각 level을 생성한다. 

이후 인접한 피라미드 간 차이(residual)를 계산하여 라플라시안 피라미드를 만들고, 이를 upsampling하여 다음 Level과 크기를 맞춘 후 차이를 계산하여 라플라시안 계수를 생성한다.

이렇게 생성된 라플라시안 계수를 사용하여 이미지를 재구성할 수 있으며, 이때는 라플라시안 계수를 업샘플링하여 이전 레벨의 이미지와 합산하면서 이미지 복원을 수행한다.

즉, LAPGAN의 한 Level에서 generator가 학습하는 것은 upsampling된 이전 level의 이미지와 원본이미지간의 차이(Residual)을 학습하는 것 이라고 볼 수 있다.

$I˜k=u(I˜k+1)+h̃k+u(I˜k+1)+Gk(zk,u(I˜k+1))$

위의 식은 이미지 $I˜k$를 다음 Laplacian pyramid level의 Upsampled version인 $I˜k+1$과 생성된 잔차 이미지, $h̃k$를 사용하여 재구성하는 과정을 나타낸다. 이 식에서 $u()$는 Upsampling 연산을 나타내며, $Gk()$는 k번째 Laplacian pyramid level에서 random noise vector $zk$와 Upsampled version인 $u(I˜k+1)$을 조건으로 사용하여 잔차 이미지 $h̃k$를 생성하는 Generatvie model을 나타낸다.

즉, 이 식은 현재 Laplacian pyramid level k에서 재구성된 이미지 $I˜k$를 생성하기 위해 이전 level의 upsampled version인 $I˜k+1$과 현재 level에서 생성된 잔차 이미지 $h̃k$를 조합하는 방법을 보여준다. 이 과정은 Recursive하게 수행되며, 마지막 level에서는 단순히 noise vector $zk$를 사용하여 잔차 이미지 $I˜K$를 생성한다. 이렇게 생성된 $I˜K$는 최종적으로 원하는 이미지의 샘플링 결과물이 된다.

![스크린샷 2023-04-02 20-37-52](https://user-images.githubusercontent.com/103099516/229350406-62be19ae-bac0-4c8b-9702-0b27d92348ea.png)

* LAPGAN 모델의 샘플링 과정

주의할 점은 마지막 레벨을 제외한 모든 레벨의 모델이 현재 이미지 $I˜k+1$의 Upsampled version을 조건 변수로 사용한다는 것 이다. 또한, 이 모델은 K=3을 가진 pyramid를 사용하여 (64,64) 이미지를 샘플링하는 데 4개의 생성 모델을 사용하고 있다.

Vanilla GAN과 다른 점은, 최초의 생성을 제외하고는 모두 noise 외에 추가로 주는 입력(조건 변수, Conditino)이 있어 좀 더 정확하게, 큰 바탕에서 작은 세부사항으로(coarse to grain) 생성할 수 있다는 점이다. 한 번에 전체 이미지를 만들던 GAN에 비해 훨씬 나은 방법이라고 할 수 있다.

![스크린샷 2023-04-02 20-42-11](https://user-images.githubusercontent.com/103099516/229350610-2d185ea4-fa8d-4984-a81a-30c871abad05.png)

* LAPGAN 모델의 훈련 과정

이 모델은 저해상도 입력 이미지에 점점 높은 frequency의 detail한 정보를 추가하여 실제같은 이미지를 생성하는 데 사용된다. 학습 과정은 train dataset에서 가져온 (64,64) 크기의 이미지 $I$로 시작한다. 이 이미지를 blur처리하고 반으로 축소하여 $I1$을 만든다. 그 다음 $I1$을 두 배로 Upsampling하여 $I0$과 같은 크기의 low frequency의 이미지 $l0$를 만든다. 

$D0$ Discriminator 모델에게는 $l0$과 함께 $I0 - l0$을 계산한 high frequency의 세부 정보 $h0$이 실제 또는 생성된 샘플인지를 판별하는 입력으로 제공된다. 생성된 경우, Generative 네트워크 $G0$는 임의의 노이즈 벡터 $z0$와 $l0$을 입력 받아 생성된 high frequency의 이미지 $h̃0$를 출력한다. $G0$는 $D0$의 입력으로 사용된다. 

이렇게 생성된 $h̃_0$은 $D0$에게 실제 샘플과 같이 입력되어 판별이 된다. 이렇게 $D_0$은 $l_0$과 함께 $hk$ 또는 $h̃k$를 입력 받아 실제 또는 생성된 샘플인지 판별한다. 이렇게 만들어진 모델은 여러 단계를 거쳐 입력 이미지의 세부 정보를 추가하여 높은 해상도의 이미지를 생성한다.

# 코드
<script src="https://gist.github.com/KimSungHeon/192408b35e2f05dbe44176996a3082fc.js"></script>
