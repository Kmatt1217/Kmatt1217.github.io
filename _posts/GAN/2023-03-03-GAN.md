---
layout: posts
title: Generative Adversarial Network
categories:
 - 논문 구현
tags:
 - GAN
 - DeepLearning
 - Generative model
toc: true
toc_sticky: true
author_profile: true
sidebar:
  nav: "categories"
---


참고 논문:
[Generative Adversarial Nets.pdf](https://github.com/KimSungHeon/KimSungHeon.github.io/files/10881126/Generative.Adversarial.Nets.pdf)


Generative Adversarial Net 이란 말 처럼, GAN은 두 플레이어, 판별자(Discriminator, D)와 생성자(Generator, G)가 minmax 게임을 하는 형식으로 object function을 최적화 해, random noise로 부터 시작해 target에 점차 맞추어가는 식으로 process가 진행된다. 

Jupyter notebook 에서 작성하였다.

<script src="https://gist.github.com/KimSungHeon/f660eededd311a3df1b76c9b9b41a6e9.js"></script>

<script src="https://gist.github.com/KimSungHeon/fb978073353ea1365ed914144ccb6b4d.js"></script>

<script src="https://gist.github.com/KimSungHeon/446f9f2b3f4677ca134634a1701d1351.js"></script>
하이퍼 파라미터
  - latent_size : Generator에서 입력으로 받아들이는 image의 크기, (차후 설명하겠지만, Generator와 Discriminator를 단순히 선형회귀 nn.Linear와 LeakyRelu의 비선형 활성화 함수의 조합으로 만들었기 때문에, Generator와 Discriminator가 받은 input shape의 차원은 1이다.)
  - hidden_units : 은닉 유닛의 갯수
  - image_size : MNIST데이터를 활용할 것이고, MNIST데이터의 이미지 size는 (height=28, width=28)로써 Discriminator의 input으로 들어가기 위해 flatten시켜 1차원으로 만들어 주었다.
  - num_epochs : total training epoch 수
  - batch_size : 100 (2의 승수로 맞춰주는 게 좋지만, 100으로 해보았다.)
  
<script src="https://gist.github.com/KimSungHeon/a6081e2ca0d015a8efe0bbe34634418a.js"></script> 
차후 Generator로부터 생성될 image들을 저장하기 위한 과정.

<script src="https://gist.github.com/KimSungHeon/4daee47f3d86b0c8d4270f1d73bb97ca.js"></script>
RGB 3 채널의 이미지를 받을 경우 이미지 전처리를 위한 변환과, GrayScale 1 채널의 이미지를 받을 경우의 변환을 나누어서 code를 만들었다.
입력된 image를 Tensor로 바꾸고, 평균과 공분산을 0.5로 설정하였다.

<script src="https://gist.github.com/KimSungHeon/e5c300f118359b1f96d9c069c97a11ca.js"></script>
참고할 데이터로  pytorch에서 지원하는 FashionMNIST 데이터셋을 활용하였다.
데이터를 불러오면서 이미지 전처리를 바로 진행하였다.

<script src="https://gist.github.com/KimSungHeon/db1222de9902d678931f1307e86deae9.js"></script>
Discriminator(판별자)와 Generator(생성자)를 지정해주었다.
각 module은 nn.Linear(선형변환)와 nn.LeakyReLU(비선형 활성화 함수)의 조합으로 구성했으며, Generator의 마지막 layer는 논문에 나와있듯이 nn.Tanh 활성화 함수를 썻고,
Discriminator의 마지막 layer에서는 Generator로 부터 생성된 fake image를 Discriminator가 True(1) 또는 False(0)으로 Binary하게 식별하기 위해 nn.Sigmoid를 활용하였다.

<script src="https://gist.github.com/KimSungHeon/9b3540fad83b05f7143207c720af9ec9.js"></script>
Discriminator의 output을 보면 Binary하므로 Loss function으로 BinacryCrossEntropy을 지정하였다.
생성자의 최적화 과정과 판별자의 최적화 과정이 번갈아 순차적으로 일어나므로, 각 module별로 따로 optimizer을 설정하였다.
(처음에 learning rate를 0.01로 설정하였더니 epoch가 100이 넘어도 generator가 제대로 image를 생성하지 못 하였는데, 학습률을 너무 높게 잡아서 그랬던 것 같다. 0.0001로 대폭 낮추어 다시 진행하였다.)

<script src="https://gist.github.com/KimSungHeon/09e049a144fed3651d063ef405a5dcc7.js"></script>
Training 시 Discriminator에 들어갈 image들이 모두 전처리 되어 정규화 되어있으므로, 다시 바꾸어주기위해 unnormalize 함수를 선언하였다.
Pytorch에서 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문에 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작하기 위해 zero_grad()를 해주었다.

<script src="https://gist.github.com/KimSungHeon/38f078781907388082e396d1d2dc894f.js"></script>
최종 training code. 생성자로부터 생성되는 가짜 이미지(fake_label)와 이를 판별하기 위한 Label, 진짜 이미지(real_label)를 batch_size에 맞게 tensor로 만들어 주었다.
비지도 학습이지만, Loss 계산을 위해 임의로 만들어진 label로 큰 뜻은 없다. 다만, Real image는 torch.ones로 모든 값이 1 인 tensor로 만들었고, Fake image는 값이 0인 tensor로 design 하였다.

Loss Functino은 판별자가 생성자로부터 생성된 가짜 이미지를 얼마나 잘 식별하는지 이다. 판별자는 제대로 식별하려 하므로, 손실을 최대한 줄이려고 하고, 생성자는 판별자를 속여 판별자의 식별 성능을 저하시키려 하므로 손실을 증가시키려 할 것이다. (실제로 코드를 돌려보면 초기에는 판별자의 손실이 매우 작고, 학습이 진행됨에 따라 손실이 증가한다.)

생성자는 random noise로부터 시작해 Real image와 최대한 유사한 image를 생성해내는 정도까지 학습을 진행한다. random noise는 Gaussian Distribution으로 부터 sampling된 데이터이다. 즉, 훈련이 진행 될 수록생성자의 input으로 들어갈 데이터를 sampling 하기 위한 임의의 probability distribution이 판별자의 data generating distribution과 점차 비슷해 진다는 것이다.

한 epoch가 끝날 때 마다, 생성된 이미지를 unnormalize해서 사전에 지정한 sample 폴더에 저장하게 했다.

<script src="https://gist.github.com/KimSungHeon/60ca632e059c65a37010e65a3a6745a1.js"></script>

[Generator_Discriminator_parameters.zip](https://github.com/KimSungHeon/KimSungHeon.github.io/files/10881613/Generator_Discriminator_parameters.zip)
최종 학습이 끝난 후 만들어진 Discriminator와 Generator의 parameter값을 ckpt파일로 저장해 두었다.

**Result**
epoch 1)

![fake_images-1](https://user-images.githubusercontent.com/103099516/222711034-80e71556-2ca1-48b2-ae4b-a28d089ed28f.png)

Gaussian random noise로부터 시작하였으므로, 별 의미 없는 아주 끔찍(?)하고 깜찍(?)한 이미지가 생성되었다.



epoch 10)

![fake_images-10](https://user-images.githubusercontent.com/103099516/222711417-205706dd-b3f4-4952-bdaa-6235b93728bc.png)

epoch가 10인데도, 어느정도 이미지에 숫자 윤곽이 드러나기 시작했다.



epoch 30)

![fake_images-30](https://user-images.githubusercontent.com/103099516/222711687-817e87ac-208c-41f1-8868-ed80bb8b23e9.png)

몇몇 sample들은 확실히 구분 가능할 정도가 되었다.



epoch 200)

![fake_images-200](https://user-images.githubusercontent.com/103099516/222711899-4351e46f-3173-4987-bd9b-450ccc4653b7.png)

이미지에 가끔 보이는 흰 pixel들이 거의 없어졌고, 생성자가 제대로 이미지를 생성했다는 것을 알 수 있다. 여전히 불분명한 pixel들이 있다.
Learing Rate를 좀 더 낮추어서 진행하거나, 판별자와 생성자 module 생성시, layer을 좀 더 깊게 만들거나 Linear(선형회귀)대신 Convolution 층을 사용하면 이미지를 더 잘 생성할 수 있을 것같다.




