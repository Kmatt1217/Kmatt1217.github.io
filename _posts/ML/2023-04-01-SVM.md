---
title: SVM
categories:
 - MachineLearning
tags:
 - MachineLearning
toc: true
toc_sticky: true

use_math: true
---

# SVM?

![DycXB7YXQAAMJjI](https://user-images.githubusercontent.com/103099516/229285686-1e6388d4-0b41-4d3d-bcb6-5c41a0f48431.jpg)
 
 서포트 벡터 머신(SVM)은 어떻게 하면 두 그룹을 직선(또는 고차원평면)으로 잘 나눌수 있을까에 대한 답을 준다. 
 Vector Space에 비선형적으로 위치해 있는 data point들을 선형적으로 분류하기 위한 기법이다. 
 DataSet의 dimension을 늘려 더 높은 차원의 Vector Space에 데이터를 정렬시켜, 이를 선형적으로 구별하기 위한 Decision Boundary를 찾는 것이 주 목적이다.
 하지만 많은 경우 직선(선형)으로 두 그룹을 나누기엔 경계가 굴곡져 있는 경우가 많다. 선형 SVM을 일반화한 kernel SVM은 기괴한 경계를 만들어 낼 수 있다.
 고차원의 벡터 공간으로 Data를 옮기는데 있어서 새로운 Transformation Matrix가 필요한데, 이를 구하고 각 data point들과 연산을 해서 
 직접적인 변환된 좌표를 찾는 것이 계산 효율이 떨어지고, 차원이 높아질 수록 힘들기 때문에, Kernel Trick이라는 기법이 필요하다.

![DycaWBHXQAAQ5Lt](https://user-images.githubusercontent.com/103099516/229285679-f6201781-5953-492a-a7c7-093b413c7cb7.jpg)

Kernel Trick: 변형된 공간(앞그림 오른쪽)에서 SVM을 돌리면 선형(평면)으로 완벽하게 파란/빨간점을 분리해낼 수 있고, 이 선형의 경계선을 원래 공간(왼쪽)으로 다시 가져오면 쭈글쭈글한 경계선(non-linear)이 되어 돌아온다. 이런 트릭을 kernel trick이라고 하는데, SVM뿐만 아니라 다른 머신러닝기법에도 종종 쓰인다.

  Kernel Trick에 주로 쓰이는 Kernel Function) 
  
![스크린샷 2023-04-01 20-26-22](https://user-images.githubusercontent.com/103099516/229285983-f907e93c-8a42-471d-9944-6a2a70035b82.png)
  
  https://en.wikipedia.org/wiki/Support_vector_machine

서포트 벡터 머신을 구현하기 위해서 알아야 하기 위해서는 손실 함수를 알아야 한다.

![images_cha-suyeon_post_73c8823a-6cb5-4637-a3ad-68fc07f7e35f_image](https://user-images.githubusercontent.com/103099516/229286328-9a67113f-777b-45df-a0a2-644571fa3ac4.png)

 이진 분류 문제라고 가정하자. 두 클래스가 있으며 결정 경계는 두 클래스를 나누는 직선 or 초평면이 될 것임을 알 수 있다. 
 목적은 이 결정경계로부터 각 클래스에 속하는 점 중 가장 가까운 점 2개(개별 클래스 마다 한 개) 까지의 거리(margin)을 최대화하는 것이다.
 결정경계로부터 어느 방향에 있는 지에 따라 클래스가 구별된다고 보면 된다.
 따라서, Dataset의 각 포인트 가 Decision Boundary (결정 경계) 로부터 올바른 방향(Correct Classification)을 가지며, margin 이상의 거리를 가지게 해야 한다.
 이에 따라 손실 함수로 Hinge Loss를 사용할 수 있다. 
 
 
 하지만, 추가적은 Regularization이 없는 경우, 결정경계는 무조건 선형적으로 데이터를 구별지으려고 할 것이다. 이를 Hard margin이라고 부른다.
 추가적인 Regularization으로 Slack Variable을 도입해서, 어느정도 오분류되는 점들을 허용하며, 이에 대한 penalty를 부과하는 형식의 손실 함수를 사용할 수 있는데,
 이를 Soft Margin이라고 한다.
 Slack variable의 도입으로 인해, 손실 함수는 제약을 가지게 되고, 최적화는 Constrained하게 된다.
 이 제약식을 라그랑주 승수법을 통해 최적화를 진행해주자. 
 (라그랑주 승수법: https://en.wikipedia.org/wiki/Lagrange_multiplier)
 
  cf) 직관적인 이해:
  
   SVM에서 우리는 결정경계의 margin을 최대화 하는 것이 목표라고 할 수 있다. 허나 dataset을 완벽하게 구별짓는 것은 사실상 불가능 하므로, 어느정도의 오분류를 허용하고,
    그에 대한 penalty를 부과하는 형식으로 최적화를 진행하면된다.
     따라서 우리가 원하는 것은, 결정경계의 margin을 최대화하는 것 과 slack variable에 의한 penalty를 최소화하는 것 사이의 trade-off이다.
     이를 직접 구하는 것이 힘들기 때문에, 라그랑주 승수법을 통해 최적화하는 것이 목적식의 Lower Bound(하한)을 Maximize(최대화)하는 것이라고 이해해도 될 것 이다.
 
 
 올바르게 분류된 경우 손실은 0이며, 데이터 포인트가 올바른 방향을 가지더라도 Decision Boundary와 margin보다 적은 거리를 가질 때 부터 손실이 증가한다.
 또한 오분류 된 경우 (다른 방향을 가지는 경우)에도 손실이 증가하기 때문에, Hinge Loss를 쓰는 것이 합리적이다.
 parameter 업데이트는 Gradient Descent를 사용하기로 하자.
 
 
# 코드 구현 
사용한 dataset : [diabetes.csv](https://github.com/KimSungHeon/KimSungHeon.github.io/files/11129342/diabetes.csv)

<script src="https://gist.github.com/KimSungHeon/bec66816587db69e3c335480155f8009.js"></script>

<script src="https://gist.github.com/KimSungHeon/77dfbaf048fd6de355a1e10d467f5e12.js"></script>

<script src="https://gist.github.com/KimSungHeon/140f0cb1fb9db8f3928538f14583b888.js"></script>

![스크린샷 2023-04-01 20-43-58](https://user-images.githubusercontent.com/103099516/229286724-25cdb829-0306-4aef-997b-7282d46779ca.png)

dataset에서 datapoint가 총 768개이며, dimension(=feature)는 9 이다.
각 feature의 통계 특성을 보면 평균과 편차가 서로 다르 다는 것을 알 수 있다. 위에서 설명하였듯이, SVM에서는 data의 위치(Coordinate)가 굉장히 중요하며 이에 민감하여
data의 scale에 따라 모델의 성능이 달라질 수 있다. 따라서 dataset의 특성들을 평균과 편차를 맞춰주자.

<script src="https://gist.github.com/KimSungHeon/6cdd7277c352e4ca2e057e20ec9055f4.js"></script>

![스크린샷 2023-04-01 20-48-19](https://user-images.githubusercontent.com/103099516/229286894-3237d856-130b-439c-8c88-3aa1a124ee10.png)

모든 특성의 평균이 0이며, 표준편차가 1이 되도록 Scaling해주었다.

<script src="https://gist.github.com/KimSungHeon/e2f3e49f2756e3cc659379095f440da8.js"></script>

![download](https://user-images.githubusercontent.com/103099516/229286933-63238662-1393-413b-bc5c-2a21b6f4f660.png)

Dataset의 클래스가 균등하므로 추가적인 preprocessing은 필요하지 않을 것 같다.

<script src="https://gist.github.com/KimSungHeon/ee830b7924671d663546c1c5072172f5.js"></script>

실행 시간은 약 2.28초 가 걸렸으며, 정확도는 0.59로 약 60프로 정도 나왔다.

적절한 하이퍼 파라미터를 구해보자.

<script src="https://gist.github.com/KimSungHeon/9fc4a9dca9f450a637433aad32ec2f30.js"></script>

![스크린샷 2023-04-01 21-05-39](https://user-images.githubusercontent.com/103099516/229287634-20c209d4-eacc-4ac1-b98c-859f551c9824.png)

 

